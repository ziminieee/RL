{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziminieee/RL/blob/main/MIR_Assignment_3_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05bbdb28",
      "metadata": {
        "id": "05bbdb28"
      },
      "source": [
        "# Assignment 3\n",
        "- In this assignment, you will implement a melody-language model using Essen Folksong dataset\n",
        "- You have to submit your code in **TWO** formats:\n",
        "    - Completed Notebook with `.ipynb`\n",
        "    - A `MIR_Assignment_3_{your_student_number}.py` file that includes **ALL functions and classes you have completed**\n",
        "        - Do not include any other code except function and class\n",
        "        - Your result will be scored by an evaluation code that import this `MIR_Assignment_3_{your_student_number}.py` file\n",
        "        - So be careful not to use any global variable inside the function\n",
        "- You have to submit a report (optional) and **three** generation results of your favorite in wav files\n",
        "    - The report is optional. If you have tried other architecture for MelodyLanguage Model, you can describe the result.\n",
        "\n",
        "\n",
        "- Caution: The `assert` lines are designed to check whether basic requirements are satisfied. Even though you passed all the assert cases, it doesn't guarantee that your implementation is fully correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "64f65852",
      "metadata": {
        "id": "64f65852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de24d359-d955-47b4-9bde-ff4cb7d69d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-20 04:21:05--  https://raw.githubusercontent.com/jdasam/ant5015/2024F/MIR_Assignment_3.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29930 (29K) [text/plain]\n",
            "Saving to: ‘MIR_Assignment_3.py’\n",
            "\n",
            "\rMIR_Assignment_3.py   0%[                    ]       0  --.-KB/s               \rMIR_Assignment_3.py 100%[===================>]  29.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-20 04:21:05 (90.5 MB/s) - ‘MIR_Assignment_3.py’ saved [29930/29930]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download Assignment_3.py\n",
        "!wget https://raw.githubusercontent.com/jdasam/ant5015/2024F/MIR_Assignment_3.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "55d255bf",
      "metadata": {
        "id": "55d255bf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.set_printoptions(sci_mode=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc24cd5c",
      "metadata": {
        "id": "dc24cd5c"
      },
      "source": [
        "## 0. Prepare (Install and import library)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c8e670ff",
      "metadata": {
        "id": "c8e670ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4857bab7-129e-4a89-fd11-af515504cb66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting muspy\n",
            "  Using cached muspy-0.5.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.10/dist-packages (from muspy) (6.0.2)\n",
            "Collecting bidict>=0.21 (from muspy)\n",
            "  Using cached bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.10/dist-packages (from muspy) (1.4.2)\n",
            "Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.10/dist-packages (from muspy) (3.8.0)\n",
            "Collecting miditoolkit>=0.1 (from muspy)\n",
            "  Using cached miditoolkit-1.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting mido>=1.0 (from muspy)\n",
            "  Using cached mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: music21>=6.0 in /usr/local/lib/python3.10/dist-packages (from muspy) (9.3.0)\n",
            "Collecting pretty-midi>=0.2 (from muspy)\n",
            "  Using cached pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pypianoroll>=1.0 (from muspy)\n",
            "  Downloading pypianoroll-1.0.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from muspy) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.10/dist-packages (from muspy) (4.66.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->muspy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->muspy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->muspy) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->muspy) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->muspy) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->muspy) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->muspy) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->muspy) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->muspy) (2.8.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from music21>=6.0->muspy) (5.2.0)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.10/dist-packages (from music21>=6.0->muspy) (4.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from music21>=6.0->muspy) (10.5.0)\n",
            "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.10/dist-packages (from music21>=6.0->muspy) (24.11.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty-midi>=0.2->muspy) (1.16.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pypianoroll>=1.0->muspy) (1.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->muspy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->muspy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->muspy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->muspy) (2024.8.30)\n",
            "Downloading muspy-0.5.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.1/119.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
            "Downloading miditoolkit-1.0.1-py3-none-any.whl (24 kB)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypianoroll-1.0.4-py3-none-any.whl (26 kB)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592287 sha256=2622b3b77e2be6636ab14a63ba3cb3809977ea35ed63ccee8e4ab4c6f40cd5fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, bidict, pretty-midi, pypianoroll, miditoolkit, muspy\n",
            "Successfully installed bidict-0.23.1 miditoolkit-1.0.1 mido-1.3.3 muspy-0.5.0 pretty-midi-0.2.10 pypianoroll-1.0.4\n",
            "Start downloading Bravura font.\n",
            "Bravura font has successfully been downloaded to : /root/.muspy/musescore-general.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fluid-soundfont-gm libevdev2 libfluidsynth3 libgudev-1.0-0 libinput-bin\n",
            "  libinput10 libinstpatch-1.0-2 libmd4c0 libmtdev1 libqt5core5a libqt5dbus5\n",
            "  libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n",
            "  libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme qttranslations5-l10n\n",
            "  timgm6mb-soundfont\n",
            "Suggested packages:\n",
            "  fluid-soundfont-gs qt5-image-formats-plugins qtwayland5 jackd\n",
            "The following NEW packages will be installed:\n",
            "  fluid-soundfont-gm fluidsynth libevdev2 libfluidsynth3 libgudev-1.0-0\n",
            "  libinput-bin libinput10 libinstpatch-1.0-2 libmd4c0 libmtdev1 libqt5core5a\n",
            "  libqt5dbus5 libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n",
            "  libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme qttranslations5-l10n\n",
            "  timgm6mb-soundfont\n",
            "0 upgraded, 32 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 148 MB of archives.\n",
            "After this operation, 207 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5core5a amd64 5.15.3+dfsg-2ubuntu0.2 [2,006 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevdev2 amd64 1.12.1+dfsg-1 [39.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmtdev1 amd64 1.1.6-1build4 [14.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-common all 2.2.0-1 [54.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom9 amd64 2.2.0-1 [22.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput-bin amd64 1.20.0-1ubuntu0.3 [19.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput10 amd64 1.20.0-1ubuntu0.3 [131 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmd4c0 amd64 0.4.8-1 [42.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5dbus5 amd64 5.15.3+dfsg-2ubuntu0.2 [222 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5network5 amd64 5.15.3+dfsg-2ubuntu0.2 [731 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5gui5 amd64 5.15.3+dfsg-2ubuntu0.2 [3,722 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5widgets5 amd64 5.15.3+dfsg-2ubuntu0.2 [2,561 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5svg5 amd64 5.15.3-1 [149 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluid-soundfont-gm all 3.1-5.3 [130 MB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinstpatch-1.0-2 amd64 1.1.6-1 [240 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 timgm6mb-soundfont all 1.3-5 [5,427 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfluidsynth3 amd64 2.2.5-1 [246 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluidsynth amd64 2.2.5-1 [27.4 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-bin amd64 2.2.0-1 [13.6 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qsynth amd64 0.9.6-1 [305 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 qt5-gtk-platformtheme amd64 5.15.3+dfsg-2ubuntu0.2 [130 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qttranslations5-l10n all 5.15.3-1 [1,983 kB]\n",
            "Fetched 148 MB in 5s (30.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 32.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libqt5core5a:amd64.\n",
            "(Reading database ... 123629 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libqt5core5a_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libevdev2:amd64.\n",
            "Preparing to unpack .../01-libevdev2_1.12.1+dfsg-1_amd64.deb ...\n",
            "Unpacking libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Selecting previously unselected package libmtdev1:amd64.\n",
            "Preparing to unpack .../02-libmtdev1_1.1.6-1build4_amd64.deb ...\n",
            "Unpacking libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
            "Preparing to unpack .../03-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
            "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Selecting previously unselected package libwacom-common.\n",
            "Preparing to unpack .../04-libwacom-common_2.2.0-1_all.deb ...\n",
            "Unpacking libwacom-common (2.2.0-1) ...\n",
            "Selecting previously unselected package libwacom9:amd64.\n",
            "Preparing to unpack .../05-libwacom9_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom9:amd64 (2.2.0-1) ...\n",
            "Selecting previously unselected package libinput-bin.\n",
            "Preparing to unpack .../06-libinput-bin_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libinput10:amd64.\n",
            "Preparing to unpack .../07-libinput10_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libmd4c0:amd64.\n",
            "Preparing to unpack .../08-libmd4c0_0.4.8-1_amd64.deb ...\n",
            "Unpacking libmd4c0:amd64 (0.4.8-1) ...\n",
            "Selecting previously unselected package libqt5dbus5:amd64.\n",
            "Preparing to unpack .../09-libqt5dbus5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5network5:amd64.\n",
            "Preparing to unpack .../10-libqt5network5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../11-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../12-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../13-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../14-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../15-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../16-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../17-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../18-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../19-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libqt5gui5:amd64.\n",
            "Preparing to unpack .../20-libqt5gui5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5widgets5:amd64.\n",
            "Preparing to unpack .../21-libqt5widgets5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5svg5:amd64.\n",
            "Preparing to unpack .../22-libqt5svg5_5.15.3-1_amd64.deb ...\n",
            "Unpacking libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package fluid-soundfont-gm.\n",
            "Preparing to unpack .../23-fluid-soundfont-gm_3.1-5.3_all.deb ...\n",
            "Unpacking fluid-soundfont-gm (3.1-5.3) ...\n",
            "Selecting previously unselected package libinstpatch-1.0-2:amd64.\n",
            "Preparing to unpack .../24-libinstpatch-1.0-2_1.1.6-1_amd64.deb ...\n",
            "Unpacking libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Selecting previously unselected package timgm6mb-soundfont.\n",
            "Preparing to unpack .../25-timgm6mb-soundfont_1.3-5_all.deb ...\n",
            "Unpacking timgm6mb-soundfont (1.3-5) ...\n",
            "Selecting previously unselected package libfluidsynth3:amd64.\n",
            "Preparing to unpack .../26-libfluidsynth3_2.2.5-1_amd64.deb ...\n",
            "Unpacking libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Selecting previously unselected package fluidsynth.\n",
            "Preparing to unpack .../27-fluidsynth_2.2.5-1_amd64.deb ...\n",
            "Unpacking fluidsynth (2.2.5-1) ...\n",
            "Selecting previously unselected package libwacom-bin.\n",
            "Preparing to unpack .../28-libwacom-bin_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom-bin (2.2.0-1) ...\n",
            "Selecting previously unselected package qsynth.\n",
            "Preparing to unpack .../29-qsynth_0.9.6-1_amd64.deb ...\n",
            "Unpacking qsynth (0.9.6-1) ...\n",
            "Selecting previously unselected package qt5-gtk-platformtheme:amd64.\n",
            "Preparing to unpack .../30-qt5-gtk-platformtheme_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package qttranslations5-l10n.\n",
            "Preparing to unpack .../31-qttranslations5-l10n_5.15.3-1_all.deb ...\n",
            "Unpacking qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Setting up libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmd4c0:amd64 (0.4.8-1) ...\n",
            "Setting up fluid-soundfont-gm (3.1-5.3) ...\n",
            "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\n",
            "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\n",
            "Setting up timgm6mb-soundfont (1.3-5) ...\n",
            "Setting up libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Setting up libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Setting up libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Setting up libwacom-common (2.2.0-1) ...\n",
            "Setting up libwacom9:amd64 (2.2.0-1) ...\n",
            "Setting up libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Setting up fluidsynth (2.2.5-1) ...\n",
            "Created symlink /etc/systemd/user/default.target.wants/fluidsynth.service → /usr/lib/systemd/user/fluidsynth.service.\n",
            "Setting up libwacom-bin (2.2.0-1) ...\n",
            "Setting up libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Setting up libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Setting up qsynth (0.9.6-1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Start downloading MuseScore General soundfont.\n",
            "MuseScore General soundfont has successfully been downloaded to : /root/.muspy/musescore-general.\n"
          ]
        }
      ],
      "source": [
        "!pip install muspy\n",
        "import muspy\n",
        "\n",
        "muspy.download_bravura_font()\n",
        "'''\n",
        "You may have to install fluidsynth.\n",
        "In Colab, you can install by followign code\n",
        "'''\n",
        "!sudo apt-get install fluidsynth\n",
        "\n",
        "muspy.download_musescore_soundfont()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ac9b3ba",
      "metadata": {
        "id": "2ac9b3ba"
      },
      "source": [
        "## Problem 1: Understanding and Implementing RNN (15 pts)\n",
        "- Recurrent neural network is a typical choice for handling sequential data with a neural network\n",
        "- In this problem, you have to implement a Vanilla RNN\n",
        "    - For each time step $t$, RNN takes two inputs\n",
        "        - $x_t$, which is an input vector of time step $t$\n",
        "        - $h_{t-1}$, which is an hidden state of previous time step, $t-1$\n",
        "            - $h_{t-1}$ is also the output of RNN for previous time step $t-1$\n",
        "    - For given $x_t$ and $h_{t-1}$, RNN returns $h_t$\n",
        "        - $h_t = \\tanh(\\textbf{W}x_t + \\textbf{U}h_{t-1} + b)$\n",
        "            - $\\textbf{W}$ and $\\textbf{U}$ is a trainable weight matrix of RNN\n",
        "            - $\\textbf{W} \\in \\mathbb{R}^{h \\times d}$, and $\\textbf{U} \\in \\mathbb{R}^{h \\times h}$, where $d$ is number of input dimension and $h$ is number of hidden state dimension\n",
        "                - This means that $\\textbf{W}$ is a matrix with real numbers and size of $\\text{num\\_hidden\\_dim} \\times \\text{num\\_input\\_dim}$\n",
        "                - and $\\textbf{U}$ is a matrix with real numbers and size of $\\text{num\\_hidden\\_dim}\\times \\text{num\\_hidden\\_dim}$  \n",
        "            \n",
        " - The output of fully connected layer (`nn.Linear`) for a given input vector $x$ is as below:\n",
        "     - $\\text{output} = \\textbf{W}x+b$\n",
        "     - Where $\\textbf{W}$ is a weight matrix and $b$ is a bias vector\n",
        "     - Both $\\textbf{W}$ and $b$ are trainable parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b050d35e",
      "metadata": {
        "id": "b050d35e"
      },
      "source": [
        "### Problem 1.1: Calculating Forward Propagation of RNN\n",
        "- Based on the example above, implement the forward propagation of uni-directional, single layer vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3564e530",
      "metadata": {
        "id": "3564e530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95215d94-a711-48a7-f13b-da404fac62f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example_weight_for_hidden_to_hidden: \n",
            " tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920],\n",
            "        [-0.3160, -2.1152,  0.3223, -1.2633,  0.3500,  0.3081],\n",
            "        [ 0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959],\n",
            "        [ 0.5667,  0.7935,  0.4397,  0.1124,  0.6408,  0.4412],\n",
            "        [-0.2159, -0.7425,  0.5627,  0.2596,  0.5229,  2.3022],\n",
            "        [-1.4689, -1.5867,  1.2032,  0.0845, -1.2001, -0.0048]])\n",
            "example_weight_for_input_to_hidden: \n",
            " tensor([[-0.2303, -0.3918, -0.4731],\n",
            "        [ 0.3356,  1.5091,  2.0820],\n",
            "        [ 1.7067,  2.3804, -1.1256],\n",
            "        [-0.3170, -0.1407,  0.8058],\n",
            "        [ 0.3276, -0.7607, -1.5991],\n",
            "        [ 0.0185, -0.7504,  0.1854]])\n",
            "example_bias: \n",
            " tensor([-0.6776,  1.0422, -1.9513,  0.4186,  3.3214,  0.8764])\n",
            "example_input_sequence: \n",
            " tensor([[ 0.3446,  0.5199, -2.6133],\n",
            "        [-1.6965, -0.2282,  0.2800],\n",
            "        [ 0.0732,  1.1133,  0.3380],\n",
            "        [ 0.4544,  0.4569, -0.8654],\n",
            "        [ 0.7813, -0.9268,  0.2064],\n",
            "        [-0.3334, -0.0729, -0.0340],\n",
            "        [ 0.9625,  0.3492, -0.9215],\n",
            "        [-0.0562, -0.7015,  1.0367],\n",
            "        [ 1.9218, -0.4025,  0.1239],\n",
            "        [ 1.1648,  0.9234,  1.3873],\n",
            "        [ 1.3750,  0.6596, -0.8048],\n",
            "        [ 0.5656,  0.6104,  0.4669],\n",
            "        [ 1.9507, -1.0631,  1.1404],\n",
            "        [-0.0899, -0.5940, -1.2439],\n",
            "        [-0.1021, -1.0335, -0.1434],\n",
            "        [-0.3173,  0.9671, -0.9911],\n",
            "        [ 0.3016, -0.1073,  0.9985],\n",
            "        [-0.4987,  0.9910, -0.7777],\n",
            "        [ 0.3140,  0.2133, -0.1201],\n",
            "        [ 0.3605, -0.3140, -1.0787]])\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Don't change this cell\n",
        "'''\n",
        "example_input_size = 3\n",
        "example_hidden_size = 6\n",
        "example_sequence_length = 20\n",
        "\n",
        "torch.manual_seed(0)\n",
        "example_weight_for_hidden_to_hidden = torch.randn([example_hidden_size, example_hidden_size])\n",
        "example_weight_for_input_to_hidden = torch.randn([example_hidden_size, example_input_size])\n",
        "example_bias = torch.randn([example_hidden_size])\n",
        "example_input_sequence = torch.randn([example_sequence_length, example_input_size])\n",
        "\n",
        "print('example_weight_for_hidden_to_hidden: \\n',example_weight_for_hidden_to_hidden)\n",
        "print('example_weight_for_input_to_hidden: \\n',example_weight_for_input_to_hidden)\n",
        "print('example_bias: \\n',example_bias)\n",
        "print('example_input_sequence: \\n',example_input_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9752e354",
      "metadata": {
        "id": "9752e354"
      },
      "outputs": [],
      "source": [
        "def rnn_single_step(current_input:torch.Tensor, prev_hidden:torch.Tensor, hh_weight:torch.Tensor, ih_weight:torch.Tensor, bias:torch.Tensor) -> torch.Tensor:\n",
        "  '''\n",
        "  This function\n",
        "\n",
        "  Arguments:\n",
        "    current_input: Input vector of the current time step. Has a shape of [input_dimension]\n",
        "    prev_hidden: Hidden state from the previous time step. Has a shape of [hidden_dimension]\n",
        "    hh_weight: Weight matrix for from hidden state to hidden state. Has a shape of [hidden_dimension, hidden_dimension]\n",
        "    ih_weight: Weight matrix for from current input to hidden state. Has a shape of [input_dimension, hidden_dimension]\n",
        "    bias: Bias of RNN. Has a shape of [hidden_dimension]\n",
        "\n",
        "  Outputs:\n",
        "    current hidden: Updated hidden state for the current time step. Has a shape of [hidden_dimension]\n",
        "\n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "  new_hidden = torch.matmul(hh_weight, prev_hidden) + torch.matmul(ih_weight, current_input) + bias\n",
        "  current_hidden = torch.tanh(new_hidden)\n",
        "\n",
        "  return current_hidden\n",
        "\n",
        "\n",
        "def initialize_hidden_state_for_single_batch(hidden_dim:int) -> torch.Tensor:\n",
        "  '''\n",
        "  This function returns zero Tensor for a given hidden dimension. This function assumes that the RNN uses single layer and single direction.\n",
        "\n",
        "  Argument\n",
        "    hidden_dim\n",
        "\n",
        "  Return\n",
        "    initial_hidden_state: Has a shape of [hidden_dim]\n",
        "\n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "  initial_hidden_state = torch.zeros(hidden_dim)\n",
        "\n",
        "  return initial_hidden_state\n",
        "\n",
        "\n",
        "initial_hidden = initialize_hidden_state_for_single_batch(example_hidden_size)\n",
        "assert initial_hidden.shape == torch.Size([example_hidden_size])\n",
        "\n",
        "single_output = rnn_single_step(example_input_sequence[0], initial_hidden, example_weight_for_hidden_to_hidden, example_weight_for_input_to_hidden, example_bias)\n",
        "assert torch.allclose(single_output, torch.Tensor([ 0.2690, -0.9982,  0.9929, -0.9535,  1.0000,  0.0081]), atol=1e-4), 'Your output is not correct. Please check your code.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b93b86e8",
      "metadata": {
        "id": "b93b86e8"
      },
      "outputs": [],
      "source": [
        "def rnn_for_entire_timestep(input_seq:torch.Tensor, prev_hidden:torch.Tensor, hh_weight:torch.Tensor, ih_weight:torch.Tensor, bias:torch.Tensor) -> tuple:\n",
        "  '''\n",
        "  This function returns the output of RNN for the given 'input_seq', for the given RNN's parameters (hh_weight, ih_weight, and bias)\n",
        "\n",
        "  Arguments:\n",
        "    input_seq: Sequence of input vector. Has a shape of [number_of_timestep, input_dimension]\n",
        "    prev_hidden: Hidden state from the previous time step. Has a shape of [hidden_dimension]\n",
        "    hh_weight: Weight matrix for from hidden state to hidden state. Has a shape of [hidden_dimension, hidden_dimension]\n",
        "    ih_weight: Weight matrix for from current input to hidden state. Has a shape of [input_dimension, hidden_dimension]\n",
        "    bias: Bias of RNN. Has a shape of [hidden_dimension]\n",
        "\n",
        "\n",
        "  Return: tuple (output, final_hidden_state)\n",
        "    output (torch.Tensor): Sequence of output hidden state of RNN along input timesteps. Has a a shape of [number_of_timestep, hidden_dimension]\n",
        "    final_hidden_state (torch.Tensor): Hidden state of RNN of the last time step. Has a a shape of [hidden_dimension]\n",
        "\n",
        "  TODO: Complete this function using your 'rnn_single_step()'\n",
        "  '''\n",
        "  outputs = []\n",
        "\n",
        "  for current_input in input_seq:\n",
        "    current_hidden = rnn_single_step(current_input, prev_hidden, hh_weight, ih_weight, bias)\n",
        "    outputs.append(current_hidden)\n",
        "    prev_hidden = current_hidden\n",
        "\n",
        "  output = torch.stack(outputs)\n",
        "\n",
        "  final_hidden_state = prev_hidden\n",
        "\n",
        "  return output, final_hidden_state\n",
        "\n",
        "total_output = rnn_for_entire_timestep(example_input_sequence, initial_hidden, example_weight_for_hidden_to_hidden, example_weight_for_input_to_hidden, example_bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a6557792",
      "metadata": {
        "id": "a6557792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b635110d-a216-44a0-8675-01089047f4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed the test cases\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test case\n",
        "'''\n",
        "\n",
        "assert isinstance(total_output, tuple) and len(total_output)==2, \"RNN's output has to be tuple of two tensors\"\n",
        "assert isinstance(total_output[0], torch.Tensor), 'Hidden states has to be a tensor'\n",
        "assert torch.allclose(total_output[0][6], torch.tensor([ 0.8273,  0.5121, -0.5701, -0.9566,  0.9984,  0.5125]), atol= 1e-4), f\"Output value is different: {total_output[0][6]}\"\n",
        "assert torch.allclose(total_output[1], torch.tensor([-0.2121, -0.9892, -0.9953,  0.7993,  1.0000, -0.9995]), atol=1e-4), f\"Output value is different: {total_output[1]}\"\n",
        "\n",
        "print(\"Passed the test cases\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb3661e",
      "metadata": {
        "id": "9fb3661e"
      },
      "source": [
        "## Problem 2: Understanding Embedding Layer (10 pts)\n",
        "- Embedding Layer takes categorical indices and return corresponding vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1dc07eca",
      "metadata": {
        "id": "1dc07eca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13261105-4f70-403b-e41d-829d4b585ee3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[3, 5],\n",
              "          [2, 1]],\n",
              " \n",
              "         [[0, 9],\n",
              "          [3, 1]],\n",
              " \n",
              "         [[1, 0],\n",
              "          [3, 6]]]),\n",
              " tensor([[[[-1.2633,  0.3500,  0.3081],\n",
              "           [-0.1116, -0.6136,  0.0316]],\n",
              " \n",
              "          [[-0.3160, -2.1152,  0.3223],\n",
              "           [-0.4339,  0.8487,  0.6920]]],\n",
              " \n",
              " \n",
              "         [[[-1.1258, -1.1524, -0.2506],\n",
              "           [ 0.0525,  0.5229,  2.3022]],\n",
              " \n",
              "          [[-1.2633,  0.3500,  0.3081],\n",
              "           [-0.4339,  0.8487,  0.6920]]],\n",
              " \n",
              " \n",
              "         [[[-0.4339,  0.8487,  0.6920],\n",
              "           [-1.1258, -1.1524, -0.2506]],\n",
              " \n",
              "          [[-1.2633,  0.3500,  0.3081],\n",
              "           [-0.4927,  0.2484,  0.4397]]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "class CustomEmbeddingLayer(nn.Module):\n",
        "  def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "    super().__init__()\n",
        "    self.weight = torch.randn(num_embeddings, embedding_dim)\n",
        "\n",
        "  def forward(self, x:torch.LongTensor):\n",
        "    '''\n",
        "    Argument\n",
        "      x: torch.LongTensor of arbitrary shape, where each element represent categorical index smaller than self.num_embeddings\n",
        "\n",
        "    Return\n",
        "      out (torch.Tensor): torch.FloatTensor with [shape of x, self.embedding_dim]\n",
        "\n",
        "    TODO: Complete this function using self.weight\n",
        "    '''\n",
        "    out = self.weight[x]\n",
        "\n",
        "    return out\n",
        "\n",
        "torch.manual_seed(0)\n",
        "custom_embedding_layer = CustomEmbeddingLayer(10, example_input_size)\n",
        "random_categorical_input = torch.randint(0,10, [3, 2, 2])\n",
        "out = custom_embedding_layer(random_categorical_input)\n",
        "random_categorical_input, out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9d1b7d2",
      "metadata": {
        "id": "f9d1b7d2"
      },
      "source": [
        "## Problem 3: Dataset (20 pts)\n",
        "- You have to declare a path for saving dataset\n",
        "- The dataset has vocabulary information\n",
        "    - For both pitch and duration, we added `'start'` and `'end'` token\n",
        "    - This helps a language model to start the generation or end the generation\n",
        "- You have to implment `__getitem__` of this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b466ebe",
      "metadata": {
        "id": "8b466ebe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3586cc-bb41-439c-c005-a0f3829ca3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "your_path = 'essen_folk/'\n",
        "'''\n",
        "You can download the dataset like this, but it will take too much time in Colab\n",
        "\n",
        "essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
        "essen.convert()\n",
        "'''\n",
        "!pip install --upgrade gdown\n",
        "!gdown 1HMHgPifMFgRtIiLJsTb3ULqbxJx4xpQY # If it doesn't work, you have to upgrade gdown by !pip install --upgrade gdown\n",
        "\n",
        "# Following code will automatically unzip te dataset to essen_folk/\n",
        "!unzip -oq essen_converted.zip  # option: overwrite, quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba4a2fe",
      "metadata": {
        "id": "4ba4a2fe",
        "outputId": "cd831c26-c64e-4b5a-f9e2-cee427df8967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skip downloading as the `.muspy.success` file is found.\n",
            "Skip extracting as the `.muspy.success` file is found.\n",
            "Skip conversion as the `.muspy.success` file is found.\n"
          ]
        }
      ],
      "source": [
        "class MelodyDataset:\n",
        "  def __init__(self, muspy_dataset, vocabs=None):\n",
        "    '''\n",
        "    The dataset takes vocabs as an argument. If vocabs is None, the dataset will automatically generate vocabs from the given dataset.\n",
        "    This is useful when you want to use the same vocabs for training and test dataset.\n",
        "\n",
        "    '''\n",
        "    self.dataset = muspy_dataset\n",
        "\n",
        "    if vocabs is None:\n",
        "      '''\n",
        "      Even though you don't have to add 'pad' when if you only use PackedSequence, we will add 'pad' to the vocabulary just in case using paddings.\n",
        "      '''\n",
        "      self.idx2pitch, self.idx2dur = self._get_vocab_info()\n",
        "      self.idx2pitch = ['pad', 'start', 'end'] + self.idx2pitch\n",
        "      self.idx2dur = ['pad', 'start', 'end'] + self.idx2dur\n",
        "      self.pitch2idx = {x:i for i, x in enumerate(self.idx2pitch)}\n",
        "      self.dur2idx = {x:i for i, x in enumerate(self.idx2dur)}\n",
        "\n",
        "    else:\n",
        "      self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx = vocabs\n",
        "\n",
        "  def _get_vocab_info(self):\n",
        "    entire_pitch = []\n",
        "    entire_dur = []\n",
        "    for note_rep in self.dataset:\n",
        "      pitch_in_piece = note_rep[:, 1]\n",
        "      dur_in_piece = note_rep[:, 2]\n",
        "      entire_pitch += pitch_in_piece.tolist()\n",
        "      entire_dur += dur_in_piece.tolist()\n",
        "    return list(set(entire_pitch)), list(set(entire_dur))\n",
        "\n",
        "  def get_vocabs(self):\n",
        "    return self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx:int):\n",
        "    '''\n",
        "    This dataset class returns melody information as a tensor with shape of [num_notes, 2 (pitch, duration)].\n",
        "\n",
        "    To train a melody language model, you have to provide a sequence of original note, and a sequence of next note for given original note.\n",
        "    In other word, melody[i+1] has to be the shifted_melody[i], so that melody[i]'s next note can be retrieved by shifted_melody[i]\n",
        "    (Remember, language model is trained to predict the next upcoming word)\n",
        "\n",
        "    Also, to make genration easier, we usually add 'start' token at the beginning of sequence, and 'end' token at the end of the sequence.\n",
        "    With these tokens, we can make the model recognize where is the start and end of the sequence explicitly.\n",
        "\n",
        "    You have to add these tokens to the note sequence at this step.\n",
        "\n",
        "    Argument:\n",
        "      idx (int): Index of data sample in the dataset\n",
        "\n",
        "    Returns:\n",
        "      melody (torch.LongTensor): Sequence of [categorical_index_of_pitch, categorical_index_of_duration]\n",
        "                                 Has a shape of [1 (start_token) + num_notes, 2 (pitch, dur)].\n",
        "                                 The first element of the sequence has to be the index for 'start' token for both pitch and duration.\n",
        "                                 The melody should not include 'end' token\n",
        "                                 (Because we don't have to predict next note if we know that current note is 'end' token)\n",
        "      shifted_melody (torch.LongTensor): Sequence of [categorical_index_of_pitch, categorical_index_of_duration]\n",
        "                                         Has a shape of [num_notes + 1 (end_token), 2 (pitch, dur)]\n",
        "                                         The i'th note of shifted melody has to be the same with (i+1)'th note of melody\n",
        "                                         The shifted melody should not include 'start' token\n",
        "                                         (Because we never get a 'start' token after a note)\n",
        "\n",
        "    TODO: Complete this function\n",
        "    '''\n",
        "\n",
        "    return\n",
        "\n",
        "your_path = 'essen_folk/'\n",
        "essen = muspy.EssenFolkSongDatabase(your_path, download_and_extract=True)\n",
        "essen.convert()\n",
        "\n",
        "essen_entire = essen.to_pytorch_dataset(representation='note')\n",
        "essen_split = essen.to_pytorch_dataset(representation='note', splits=(0.8, 0.1, 0.1), random_state=0)\n",
        "entire_set = MelodyDataset(essen_entire)\n",
        "\n",
        "vocabs = entire_set.get_vocabs()\n",
        "train_set = MelodyDataset(essen_split['train'], vocabs=vocabs)\n",
        "valid_set = MelodyDataset(essen_split['validation'], vocabs=vocabs)\n",
        "test_set = MelodyDataset(essen_split['test'], vocabs=vocabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0372bb7",
      "metadata": {
        "id": "c0372bb7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "To check the MelodyDataset implementation\n",
        "'''\n",
        "\n",
        "assert len(train_set[0]) == 2, \"You have to return two variables at __getitem__\"\n",
        "assert train_set[0][0].shape == train_set[0][1].shape, \"Shape of Melody and Shifted melody has to be the same\"\n",
        "\n",
        "assert (train_set[0][0][0] == torch.LongTensor([1, 1])).all(), \"You have to add start token at the beginning of melody\"\n",
        "assert (train_set[0][1][-1] == torch.LongTensor([2, 2])).all(), \"You have to add end token at the end of melody\"\n",
        "\n",
        "assert (train_set[0][0][-1] == torch.LongTensor([15, 29])).all(), \"Last part of melody must not include the end token\"\n",
        "assert (train_set[0][1][0] == torch.LongTensor([27, 19])).all(),  \"First part of shifted melody must not include the start token\"\n",
        "\n",
        "assert (train_set[20][0][1:] == train_set[20][1][:-1]).all(), \"Check the melody shift\"\n",
        "\n",
        "print(\"Passed test cases\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4673c342",
      "metadata": {
        "id": "4673c342"
      },
      "source": [
        "## PackedSequence\n",
        "- After implementing Dataset, we have to declare DataLoader that groups several training samples as a single batch\n",
        "- However, we cannot batchify the melodies in straightforward way, because the length of each melody is different\n",
        "- In this problem, you will learn about how to handle sequences of different length as a batch\n",
        "\n",
        "- You can also refer [a video lecture](https://youtu.be/IQf1zu6jdCU) in Korean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83754613",
      "metadata": {
        "scrolled": false,
        "id": "83754613"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "'''\n",
        "This cell will make error, because the length of each sample is different to each other\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8)\n",
        "batch = next(iter(train_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58708cbf",
      "metadata": {
        "id": "58708cbf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "To handle that problem, you have to make your collate function\n",
        "'''\n",
        "def your_collate_function(raw_batch):\n",
        "  '''\n",
        "  You can make your own function to handle the batch\n",
        "  '''\n",
        "\n",
        "  return raw_batch[0] # This returns the first melody of each batch. So it will avoid the error, but it doesn't do proper batchifying\n",
        "\n",
        "batch_size = 8\n",
        "raw_batch = [train_set[i] for i in range(batch_size)] # This is the input for the collate function\n",
        "batch = your_collate_function(raw_batch)\n",
        "\n",
        "'''\n",
        "This is what the 'collate_fn' does in DataLoader\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=your_collate_function)\n",
        "batch_by_loader = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b0a209",
      "metadata": {
        "id": "86b0a209"
      },
      "source": [
        "#### Pad Sequence and Pack Sequence\n",
        "In PyTorch, there are two ways to batchify a group of sequence with different length.\n",
        "- `torch.nn.utils.rnn.pad_sequence`\n",
        "    - This function takes list of tensors with different length and return padded sequence\n",
        "    - Padding is adding some constant number as a PAD token to match the length of short sequence to the maximum length\n",
        "        - e.g. If there are sequence of length (3,7,4), we can add 4 zeros to sequence with length 3, 3 zeros to sequence with length 4 to make them length 7\n",
        "    - In default, we use 0 for padding (zero padding)\n",
        "    - The result\n",
        "- `torch.nn.utils.rnn.pack_sequence`\n",
        "    - pad_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f2a21e",
      "metadata": {
        "id": "f5f2a21e"
      },
      "source": [
        "Below cells show the example of pad sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24424475",
      "metadata": {
        "id": "24424475"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, PackedSequence\n",
        "short = torch.Tensor([0, 1, 2])\n",
        "long = torch.Tensor([3, 6, 8, 12, 1, 2, 3])\n",
        "middle = torch.Tensor([2, 3, 4, 3, 0])\n",
        "\n",
        "pad_sequence([short, long, middle], batch_first=False)  # T x N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99aec137",
      "metadata": {
        "scrolled": false,
        "id": "99aec137"
      },
      "outputs": [],
      "source": [
        "# Default value of batch_first in pad_sequence is False.\n",
        "# So you have to always be careful not to miss batch_first=True in pad_sequence, if you use batch_first=True for your RNN layer.\n",
        "pad_sequence([short, long, middle], batch_first=True)  # N x T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b420b0",
      "metadata": {
        "id": "f4b420b0"
      },
      "source": [
        "1) However, the problem is that you can't figure out whether the 0 at the end of each sequence is a padded one, or was included in the original sequence\n",
        "- e.g. `[2, 3, 4, 3, 0]` becomes `[ 2,  3,  4,  5,  0,  0,  0]`. Now we don't know how many zeros were added for padding\n",
        "\n",
        "2) Also, if you run RNN for this padded sequence, RNN will calculate for the padded part also.\n",
        "- RNN doesn't know whether it is padded data, or existing data\n",
        "- This makes computation slower\n",
        "\n",
        "3) If you want to use bi-directional, which also reads the sequence from backward, paddings can make the result different.\n",
        "\n",
        "To solve this issue, we use PackedSequence, by using `pack_sequence`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f553adab",
      "metadata": {
        "scrolled": true,
        "id": "f553adab"
      },
      "outputs": [],
      "source": [
        "packed_sequence = pack_sequence([short, long, middle], enforce_sorted=False)\n",
        "packed_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c04cfe",
      "metadata": {
        "id": "d4c04cfe"
      },
      "source": [
        "`PackedSequence` has `data` and `batch_sizes`\n",
        "- `data` contains the flattened value of given batch\n",
        "    - To optimize the computation, the sequences have to be sorted by descending of length\n",
        "- `batch_sizes` represents how many valid batch sample exists for each time step\n",
        "    - `[3, 3, 3, 2, 2, 1, 1]` means that there are 3 sequences for first three time steps, and then 2 sequences for next two steps, and then only 1 sequence for next two steps.\n",
        "- `sorted_indices` shows how the sorted sequences can be converted to original order.\n",
        "    - `[1,2,0]` means that\n",
        "        - the 0th sequence in the sorted sequences (the longest one) was indexed as 1 in the original input batch\n",
        "        - the 1st sequence in the sorted sequences (`middle`) was indexed as 2 in the original input batch\n",
        "        - the 2nd sequence in the sorted sequences (`short`) was index as 0 in the original input batch\n",
        "- `unsorted_indices` shows how the original sequences are sorted.\n",
        "    - `[2,0,1]` means that\n",
        "        - the 0th sequence in the original input was sorted as 2nd in the sorted sequences\n",
        "        \n",
        "       \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc405b5",
      "metadata": {
        "id": "cdc405b5"
      },
      "source": [
        "If you feed PackedSequence to RNN (or LSTM, GRU), it will return PackedSequence with same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a65589f3",
      "metadata": {
        "id": "a65589f3"
      },
      "outputs": [],
      "source": [
        "rnn_layer = nn.GRU(1, 1)\n",
        "packed_sequence = pack_sequence([short.unsqueeze(1), long.unsqueeze(1), middle.unsqueeze(1)], enforce_sorted=False)\n",
        "out, last_hidden = rnn_layer(packed_sequence)\n",
        "\n",
        "print(f\"Type of output of RNN for PackedSequence: {type(out)}\")\n",
        "print(f\"Type of last_hidden of RNN for PackedSequence: {type(last_hidden)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89df707e",
      "metadata": {
        "id": "89df707e"
      },
      "source": [
        "- RNN or its family of PyTorch can automatically handle `PackedSequence`\n",
        "- However, other layers like `nn.Embedding` or `nn.Linear` cannot take `PackedSequence` as its input\n",
        "- There are two ways to feed `PackedSequence` to these layers\n",
        "    - First, convert PackedSequence to ordinary torch.Tensor by `torch.nn.utils.rnn.pad_packed_sequence`\n",
        "        - This will convert PackedSequence to a tensor of sequneces with same length but different padding\n",
        "    - The other way is to feed only PackedSequence.data, and then declaring new PackedSequence with the output as `data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e983f58",
      "metadata": {
        "id": "3e983f58"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This will make error, because other layers cannot handle PackedSequence\n",
        "'''\n",
        "test_linear_layer = nn.Linear(in_features=1, out_features=2)\n",
        "test_linear_layer(packed_sequence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1754e2dd",
      "metadata": {
        "id": "1754e2dd"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "One way to this is using torch.nn.utils.rnn.pad_packed_sequence to convert PackedSequence to ordinary tensor\n",
        "'''\n",
        "\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "padded_sequence, batch_lengths = pad_packed_sequence(packed_sequence)\n",
        "print(f'The padded sequence generated from packed sequence (squeezed for printing): \\n {padded_sequence.squeeze()}')\n",
        "print(f'\"pad_packed_sequence\" also returns \"batch_lengths\", to clarify the original length before the padding: \\n {batch_lengths}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7871b5da",
      "metadata": {
        "scrolled": true,
        "id": "7871b5da"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Now you can feed padded sequence to linear layer.\n",
        "'''\n",
        "\n",
        "linear_output = test_linear_layer(padded_sequence)\n",
        "print(f\"Output of feeding padded_sequence to a linear layer: {linear_output}\")\n",
        "print(\"Caution that it returns non-zero values for timestep with zero padding, because linear layer has a bias\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d104d5",
      "metadata": {
        "scrolled": true,
        "id": "71d104d5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You can make the output as a PackedSequence, by using torch.nn.utils.rnn.pack_padded_sequence\n",
        "'''\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "re_packed_sequence = pack_padded_sequence(linear_output, batch_lengths, enforce_sorted=False)\n",
        "re_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a869f708",
      "metadata": {
        "id": "a869f708"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Another way to do it is using PackedSequence.data\n",
        "'''\n",
        "\n",
        "linear_out_pack = test_linear_layer(packed_sequence.data)\n",
        "packed_sequence_after_linear = PackedSequence(linear_out_pack, packed_sequence.batch_sizes, packed_sequence.sorted_indices, packed_sequence.unsorted_indices)\n",
        "packed_sequence_after_linear"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aef97f8e",
      "metadata": {
        "id": "aef97f8e"
      },
      "source": [
        "## Problem 4: Implement pack_collate(), (20 pts)\n",
        "- Implement a collate function that returns PackedSequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518ed3d1",
      "metadata": {
        "id": "518ed3d1"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
        "\n",
        "def pack_collate(raw_batch:list):\n",
        "  '''\n",
        "  This function takes a list of data, and returns two PackedSequences\n",
        "\n",
        "  Argument\n",
        "    raw_batch: A list of MelodyDataset[idx]. Each item in the list is a tuple of (melody, shifted_melody)\n",
        "               melody and shifted_melody has a shape of [num_notes (+1 if you don't consider \"start\" and \"end\" token as note), 2]\n",
        "  Returns\n",
        "    packed_melody (torch.nn.utils.rnn.PackedSequence)\n",
        "    packed_shifted_melody (torch.nn.utils.rnn.PackedSequence)\n",
        "\n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "  return\n",
        "\n",
        "raw_batch = [train_set[i] for i in range(batch_size)]\n",
        "packed_melody, packed_shifted_melody = pack_collate(raw_batch)\n",
        "packed_melody, packed_shifted_melody"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff1e46bf",
      "metadata": {
        "id": "ff1e46bf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test whether you have implemented pack_collate correctly\n",
        "'''\n",
        "\n",
        "assert isinstance(packed_melody, PackedSequence)\n",
        "assert isinstance(packed_shifted_melody, PackedSequence)\n",
        "\n",
        "assert packed_melody.data.shape==packed_shifted_melody.data.shape\n",
        "\n",
        "print(\"Passed all the test cases\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d182eeb",
      "metadata": {
        "scrolled": true,
        "id": "6d182eeb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=128, collate_fn=pack_collate, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=128, collate_fn=pack_collate, shuffle=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73263352",
      "metadata": {
        "id": "73263352"
      },
      "source": [
        "## Problem 5: Define Melody Language Model (25 pts)\n",
        "- In this problem, you have to define a Language Model for model\n",
        "    - It is almost same as an ordinary language model for natural language processing\n",
        "    - The key difference is that the melody language model has to predict pitch **and** duration\n",
        "- Complete the model step-by-step\n",
        "    - Complete each function and test the function with the cells below\n",
        "    - `get_concat_embedding()` makes concatenated embedding for each note given pitch and duration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893efba3",
      "metadata": {
        "scrolled": true,
        "id": "893efba3"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import PackedSequence\n",
        "\n",
        "class MelodyLanguageModel(nn.Module):\n",
        "  def __init__(self, hidden_size, embed_size, vocabs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.idx2pitch, self.idx2dur, self.pitch2idx, self.dur2idx = vocabs\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embed_size = embed_size\n",
        "    self.num_pitch = len(self.idx2pitch)\n",
        "    self.num_dur = len(self.idx2dur)\n",
        "    self.num_layers = 3\n",
        "\n",
        "\n",
        "    '''\n",
        "    TODO: Declare four modules. Please follow the name strictly.\n",
        "      1) self.pitch_embedder: nn.Embedding layer that embed pitch category index to a vector with size of 'embed_size'\n",
        "      2) self.dur_embedder = nn.Embedding layer that embed duration category index to a vector with size of 'embed_size'\n",
        "      3) self.rnn = nn.GRU layer that takes concatenated_embedding and has a hidden size of 'hidden_size', num_layers of self.num_layers, and batch_first=True\n",
        "      4) self.final_layer = nn.Linear layer that takes self.rnn's output and convert it to logits (that can be used as input of softmax) of pitch + duration\n",
        "    '''\n",
        "\n",
        "  def get_concat_embedding(self, input_seq):\n",
        "    '''\n",
        "    This function returns concatenated pitch embedding and duration embedding for a given input seq\n",
        "\n",
        "    Arguments:\n",
        "      input_seq: A batch of melodies represented as a sequence of vector (pitch_idx, dur_idx).\n",
        "                 Has a shape of [num_batch, num_timesteps (num_notes), 2(pitch, dur)], or [num_timesteps (num_notes), 2]\n",
        "                 벡터 (pitch_idx, dur_idx)의 시퀀스로 표현된 멜로디들의 집합으로 이루어진 배치.\n",
        "                 Shape은 [배치 샘플 수, 타임스텝의 수 (==음표의 수), 2 (음고, 길이)] 혹은 [타임스텝의 수 (num_notes), 2]\n",
        "    Return:\n",
        "      concat_embedding: A batch of sequence of concatenated embedding of pitch embedding and duration embedding.\n",
        "                        Has a shape of [num_batch, num_timesteps (num_notes), embedding_size * 2]\n",
        "                        Each vector of time t is [pitch_embedding ; duration_embedding] (concatenation)\n",
        "\n",
        "                        pitch embedding is the output of an nn.Embedding layer of given note pitch index\n",
        "                        duration embedding is the output of an nn.Embedding layer of given note duration index\n",
        "\n",
        "    TODO: Complete this function using self.pitch_embedder and self.dur_embedder\n",
        "    You can use torch.cat to concatenate two tensors or vectors\n",
        "    '''\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "  def initialize_rnn(self, batch_size: int) -> torch.Tensor :\n",
        "    '''\n",
        "    This function returns initial hidden state for self.rnn for given batch_size\n",
        "\n",
        "    Argument\n",
        "      batch_size (int):\n",
        "\n",
        "    Return\n",
        "      initial_hidden_state (torch.Tensor):\n",
        "    '''\n",
        "\n",
        "    return torch.zeros([self.num_layers, batch_size, self.hidden_size])\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input_seq:torch.LongTensor):\n",
        "    '''\n",
        "    Forward propgation of Melody Language Model.\n",
        "\n",
        "    Argument\n",
        "      input_seq: A batch of melodies represented as a sequence of vector (pitch_idx, dur_idx).\n",
        "                 Has a shape of [num_batch, num_timesteps (num_notes), 2(pitch, dur)], or can be a PackedSequence\n",
        "                 벡터 (pitch_idx, dur_idx)의 시퀀스로 표현된 멜로디들의 집합으로 이루어진 배치.\n",
        "                 Shape은 [배치 샘플 수, 타임스텝의 수 (==음표의 수), 2 (음고, 길이)] 혹은 PackedSequence.\n",
        "\n",
        "    Output\n",
        "      pitch_dist: Probability distribution of pitch of next upcoming note for each timestep 't'.\n",
        "                  Has a shape of [num_batch, numtimesteps, self.num_pitch]\n",
        "                매 타임 스텝 t에 대해, 그 다음에 등장할 음표 음고의 확률 분포\n",
        "      dur_dist: Probability distribution of duration of next upcoming note for each timestep 't'.\n",
        "                Has a shape of [num_batch, numtimesteps, self.num_dur]\n",
        "                매 타임 스텝 t에 대해, 그 다음에 등장할 음표 길이의 확률 분포\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    '''\n",
        "    TODO: Complete this function. You have to handle both cases: input_seq as ordinary Tensor / input_seq as PackedSequence\n",
        "    If the input_seq is PackedSequence, return PackedSequence\n",
        "\n",
        "\n",
        "    input_seq → self.get_concat_embedding → self.rnn → self.final_layer → torch.softmax for [pitch, duration]\n",
        "\n",
        "    Follow the instruction\n",
        "    '''\n",
        "\n",
        "    if isinstance(input_seq, torch.Tensor): # If input is an ordinary tensor\n",
        "\n",
        "      # 1. Get concatenated_embeddings using self.get_concat_embedding\n",
        "\n",
        "      # 2. Put concatenated_embeddings to self.rnn.\n",
        "      # Remember: RNN, GRU, LSTM returns two outputs\n",
        "\n",
        "      # 3. Put rnn's output with a shape of [num_batch, num_timestep, hidden_size] to self.final_layer\n",
        "\n",
        "      # 4. Convert logits (output of self.final_layer) to pitch probability and duration probability\n",
        "      # Caution! You have to get separately softmax-ed pitch and duration\n",
        "      # Because you have to pick one pitch and one duration from the probability distribution\n",
        "\n",
        "      pass # Delete this after you complete the code\n",
        "    elif isinstance(input_seq, PackedSequence):\n",
        "      # 1. Get concatenated_embeddings using self.get_concat_embedding\n",
        "      # To get concatenated_embeddings, You have to either pad_packed_sequence(input_seq, batch_first=True)\n",
        "      # Or use input_seq.data, and then make new PackedSequence using concatenated_embeddings as data, and copy batch_lengths, sorted_indices, unsorted_indices.\n",
        "\n",
        "      # 2. Put concatenated embedding to self.rnn\n",
        "\n",
        "      # 3. Put rnn output to self.final_layer to get probability logit for pitch and duration\n",
        "      # Again, rnn's output is PackedSequence so you have to handle it\n",
        "\n",
        "      # 4. Convert logits to pitch probability and duration probability\n",
        "      # Caution! You have to get separately softmax-ed pitch and duration\n",
        "      # Because you have to pick one pitch and one duration from the probability distribution\n",
        "\n",
        "      # Return output as PackedSequence\n",
        "      pass # Delete this after you complete the code\n",
        "    else:\n",
        "      print(f\"Unrecognized input type: {type(input_seq)}\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "hidden_size = 64\n",
        "embed_size = 40\n",
        "\n",
        "model = MelodyLanguageModel(hidden_size, embed_size, entire_set.get_vocabs())\n",
        "model(batch[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "108a8117",
      "metadata": {
        "scrolled": true,
        "id": "108a8117"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test model.get_concat_embedding\n",
        "'''\n",
        "batch = next(iter(train_loader))\n",
        "melody, shifted_melody = batch\n",
        "padded_melody, _ = pad_packed_sequence(melody, batch_first=True)\n",
        "\n",
        "concat_embedding = model.get_concat_embedding(padded_melody)\n",
        "print(f'Your concat_embedding: \\n{concat_embedding}')\n",
        "\n",
        "assert concat_embedding.shape[:-1] == padded_melody.shape[:-1], \"Num_batch and num_timestep of concat_embedding has to be the same with input melody\"\n",
        "assert concat_embedding.shape[2] == embed_size * 2, \"Error in size of embedding dimension\"\n",
        "assert (concat_embedding[0,0,:] == concat_embedding[1,0,:]).all(), \"Error: your embedding vectors for the same input notes are different\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd0050c6",
      "metadata": {
        "id": "cd0050c6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test code with ordinary tensor (using batch_size=1)\n",
        "'''\n",
        "\n",
        "single_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
        "single_batch = next(iter(single_loader))\n",
        "single_melody, single_shifted_melody = single_batch\n",
        "pitch_out, dur_out = model(single_melody)\n",
        "\n",
        "assert pitch_out.shape == (1,single_melody.shape[1], model.num_pitch),  \\\n",
        "          f\"Error in pitch_out.shape. Expected {1,single_melody.shape[1], model.num_pitch}, but got {pitch_out.shape}\"\n",
        "assert dur_out.shape == (1,single_melody.shape[1], model.num_dur), \\\n",
        "          f\"Error in dur_out.shape. Expected {1,single_melody.shape[1], model.num_dur}, but got {dur_out.shape}\"\n",
        "\n",
        "assert (0<pitch_out).all() and (pitch_out<1).all() and (0<dur_out).all() and (dur_out<1).all(), \\\n",
        "          \"Every output must have a value between 0 and 1 \"\n",
        "assert (torch.abs(torch.sum(pitch_out, dim=-1)-1)<1e-5).all(), \\\n",
        "          \"Sum of probability of every pitch class has to be 1\"\n",
        "assert (torch.abs(torch.sum(dur_out, dim=-1)-1)<1e-5).all(), \\\n",
        "          \"Sum of probability of every duration class has to be 1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a022349b",
      "metadata": {
        "id": "a022349b"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test code with PackedSequence\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
        "batch = next(iter(train_loader))\n",
        "melody, shifted_melody = batch\n",
        "pitch_out, dur_out = model(melody)\n",
        "\n",
        "assert isinstance(pitch_out, type(melody)) and isinstance(dur_out, type(melody)), f\"Input of model was {type(melody)} but output is {type(pitch_out)}\"\n",
        "\n",
        "assert (pitch_out.batch_sizes == melody.batch_sizes).all(), \\\n",
        "          \"batch_sizes of input and output has to be the same\"\n",
        "assert len(pitch_out.data) == len(batch[0].data), \"Number of notes in input and output has to be the same\"\n",
        "assert (torch.abs(torch.sum(pitch_out.data, dim=-1)-1)<1e-5).all(), \\\n",
        "          \"Sum of probability of every pitch class has to be 1\"\n",
        "assert (torch.abs(torch.sum(dur_out.data, dim=-1)-1)<1e-5).all(), \\\n",
        "          \"Sum of probability of every duration class has to be 1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a734ec23",
      "metadata": {
        "id": "a734ec23"
      },
      "source": [
        "## Problem 6. Implement training loop (25 pts)\n",
        "- If you have succeeded in implementing model for PackedSequence, you can implement the training loop assuming that input batch is a PackedSequence\n",
        "- If not, you can implement the training loop using batch_size=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d8d6c3",
      "metadata": {
        "scrolled": false,
        "id": "c1d8d6c3"
      },
      "outputs": [],
      "source": [
        "def get_nll_loss(prob_distribution, correct_class):\n",
        "  '''\n",
        "  This function takes predicted probability distrubtion and the corresponding correct_class.\n",
        "\n",
        "  For example,  prob_distribution = [[0.2287, 0.2227, 0.5487], [0.1301, 0.4690, 0.4010]] means that\n",
        "  for 0th data sample, the predicted probability for 0th category is 0.2287, for 1st category is 0.2227, and for 2nd category is 0.5487,\n",
        "  and for 1st data sample, the predicted probability for 0th category is 0.1301, for 1st category is 0.4690, and for 2nd category is 0.4010,\n",
        "\n",
        "  Negative Log Likelihood, which is -y*log(y_hat), can be regarded as negative log value of predicted probability for correct class (y==1).\n",
        "  If the given correct_class is [1, 2], the loss for 0th data sample becomes negative log of [0.2287, 0.2227, 0.5487][1], which is -torch.log(0.2227),\n",
        "  because the correct category for this sample was 1st category, and the predicted probability was 0.2227\n",
        "  And the loss for 1st data sample becomes negative log of [0.1301, 0.4690, 0.4010][2], which is -torch.log(0.4010),\n",
        "  because the correct category for this sample was 2nd category, and the predicted probability was 0.4010\n",
        "\n",
        "  To make implementation easy, let's assume we have 2D tensor for prob_distribution and  1D tensor for correct_class\n",
        "\n",
        "  Arguments:\n",
        "    prob_distribution (2D Tensor)\n",
        "    correct_class (1D Tensor)\n",
        "\n",
        "  Return:\n",
        "    loss (torch.Tensor): Negative log likelihood loss for every data sample in prob_distrubition. Has a same shape with correct_class\n",
        "\n",
        "  TODO: Complete this function\n",
        "\n",
        "  Caution:  Do not return the mean loss. Return loss that has same shape with correct_class\n",
        "  Try not to use for loop, or torch.nn.CrossEntropyLoss, or torch.nn.NLLLoss\n",
        "  '''\n",
        "  assert prob_distribution.dim() == 2 and correct_class.dim() == 1, \"Let's assume we only take 2D tensor for prob_distribution and 1D tensor for correct_class\"\n",
        "  # Write your code from here\n",
        "\n",
        "  return\n",
        "torch.manual_seed(0)\n",
        "prob_distribution = torch.softmax(torch.randn([10, 3]), dim=-1)\n",
        "correct_class = torch.randint(0,3, [10])\n",
        "print(f\"prob_distribution: \\n{prob_distribution}, \\n correct_class for each datasample: \\n {correct_class.unsqueeze(1)}\")\n",
        "\n",
        "loss = get_nll_loss(prob_distribution, correct_class)\n",
        "print('Loss: ', loss)\n",
        "assert (torch.abs(loss-torch.Tensor([1.5020, 0.7572, 0.4797, 0.7693, 0.4563, 0.8718, 0.7973, 1.3412, 1.6403, 0.2423]))<1e-4).all(), \"Error in loss value\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963172fe",
      "metadata": {
        "id": "963172fe"
      },
      "outputs": [],
      "source": [
        "def get_loss_for_single_batch(model, batch, device):\n",
        "  '''\n",
        "  This function takes model and batch and calculate Cross Entropy Loss for given batch.\n",
        "\n",
        "  Arguments:\n",
        "    model (MelodyLanguageModel)\n",
        "    batch (batch collated by pack_collate): Tuple of (melody_batch, shifted_melody_batch)\n",
        "    device (str): cuda or cpu. In which device to calculate the batch\n",
        "\n",
        "  Return:\n",
        "    loss (torch.Tensor): Calculated mean loss for given model and batch. Has a shape of 0D tensor\n",
        "\n",
        "  TODO: Complete this function using get_nll_loss().\n",
        "  Now you have to return the mean loss of every data sample in the batch\n",
        "\n",
        "  Caution: You have to calculate loss for pitch, and loss for duration separately.\n",
        "  Then you can take average of pitch_loss and duration_loss\n",
        "\n",
        "  Important Tip: If you are using PackedSequence, you can feed PackedSequence.data directly to get_nll_loss.\n",
        "  It makes the implementation much easier, because it doesn't need to reshape probabilty distribution and correct_class\n",
        "  '''\n",
        "\n",
        "\n",
        "  return\n",
        "\n",
        "model.to('cuda')\n",
        "batch = next(iter(train_loader))\n",
        "out = get_loss_for_single_batch(model, batch, device='cuda')\n",
        "\n",
        "assert isinstance(out, torch.Tensor), \"Return value of get_loss_for_single_batch has to be torch.Tensor\"\n",
        "assert out.dim() == 0, \"Return value of get_loss_for_single_batch has to be torch.Tensor with dim 0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aeb02b8",
      "metadata": {
        "id": "9aeb02b8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "If you have implemented the previous function correctly, this code will train the model\n",
        "'''\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DEV = 'cuda' # or cpu, but using cpu will be too slow\n",
        "model = MelodyLanguageModel(hidden_size, embed_size, entire_set.get_vocabs())\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_epochs = 30\n",
        "\n",
        "model.to(DEV)\n",
        "loss_record = []\n",
        "valid_loss_record = []\n",
        "best_valid_loss = torch.inf\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  model.train()\n",
        "  for batch in tqdm(train_loader,leave=False):\n",
        "    loss = get_loss_for_single_batch(model, batch, device=DEV)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_record.append(loss.item())\n",
        "\n",
        "  # Validation\n",
        "  with torch.inference_mode():\n",
        "    model.eval()\n",
        "    loss_for_entire_valid = 0\n",
        "    num_notes = 0\n",
        "    for batch in valid_loader:\n",
        "      loss = get_loss_for_single_batch(model, batch, device=DEV)\n",
        "      if isinstance(batch[0], PackedSequence):\n",
        "        n_note = len(batch[0].data)\n",
        "      else:\n",
        "        n_note = batch[0].shape[1]\n",
        "\n",
        "      loss_for_entire_valid += loss.item() * n_note\n",
        "      num_notes += n_note\n",
        "    valid_loss = loss_for_entire_valid/num_notes\n",
        "    if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(model.state_dict(), 'best_model.pt')\n",
        "    else:\n",
        "      torch.save(model.state_dict(), 'last_model.pt')\n",
        "    valid_loss_record.append(loss_for_entire_valid/num_notes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be25b1c",
      "metadata": {
        "id": "4be25b1c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2,1,1)\n",
        "plt.title('Training Loss')\n",
        "plt.plot(loss_record)\n",
        "plt.subplot(2,1,2)\n",
        "plt.title('Validation Loss')\n",
        "plt.plot(valid_loss_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e96ed2",
      "metadata": {
        "id": "a5e96ed2"
      },
      "source": [
        "## Problem 7: Implement Generation (25 pts)\n",
        "- In this problem, you have to generate a new melody using the trained model\n",
        "- Melody language model can generate a new sequence by sampling a new note for each timestep, and feed the generated new note again to the model to predict the next note\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3cd915d",
      "metadata": {
        "id": "a3cd915d"
      },
      "source": [
        "### Problem 7-1: Implement model inference (15 pts)\n",
        "- Inference in Language model is little bit different from an ordniary forward loop during the training.\n",
        "    - While training, you have entire sequence, from beginning to end.\n",
        "    - During the inference, you have to generate one note, and then feed it as an input for the next step\n",
        "- You have to implement given functions one by one to complete `generate()`\n",
        "- In this problem, you can assume that the model is on cpu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f553d4f7",
      "metadata": {
        "id": "f553d4f7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You can assume that model is on cpu during the Problem 7\n",
        "'''\n",
        "model.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2afa1675",
      "metadata": {
        "id": "2afa1675"
      },
      "outputs": [],
      "source": [
        "def get_initial_input_and_hidden_state(model, batch_size=1):\n",
        "  '''\n",
        "  This function generates initial input vector and hidden state for model's GRU\n",
        "\n",
        "  To generate a new sequence, you have to provide initial seed token, which is ['start', 'start'].\n",
        "  You have to make a initial vector that has [pitch_category_index_of_'start', duration_category_index_of_'start']\n",
        "\n",
        "  You also have to initial hidden state for the model's RNN.\n",
        "  In uni-directional RNN(or GRU), hidden state of RNN has to be a zero tensor with shape of (num_layers, batch_size, hidden_size)\n",
        "\n",
        "\n",
        "  Argument:\n",
        "    model (MelodyLanguageModel)\n",
        "\n",
        "  Returns:\n",
        "    initial_input_vec (torch.Tensor): Has a shape of [batch_size, 1 (timestep), 2]\n",
        "    initial_hidden (torch.Tensor): Has a shape of [num_layers, bach_size, hidden_size]\n",
        "\n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "\n",
        "  return\n",
        "\n",
        "batch_size = 2\n",
        "input_vec, initial_hidden = get_initial_input_and_hidden_state(model, batch_size=batch_size)\n",
        "print(f'input_vec: \\n{input_vec} \\n initial_hidden: \\n {initial_hidden}')\n",
        "\n",
        "assert input_vec.ndim == 3\n",
        "assert initial_hidden.ndim == 3\n",
        "assert input_vec.shape == (batch_size, 1, 2)\n",
        "assert initial_hidden.shape == (model.num_layers, batch_size, model.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589c5e91",
      "metadata": {
        "id": "589c5e91"
      },
      "source": [
        "### Hint: Sampling from distribution\n",
        "- The language model predict probability distribution of pitch and duration for  upcoming note\n",
        "- To do that, you have to know how to sample a result from a given probability distribution\n",
        "- In PyTorch, you can use `atensor.multinomial(num_samples)`\n",
        "    - In this assignment you don't have to sample more than 1, but\n",
        "    - multinomial(num_samples=100, replacement=False) means that you want to sample 100 samples without overlapping category\n",
        "        - Thus, the total class has to be larger than 100, because you cannot sample a single category multiple time\n",
        "    - multinomial(num_samples=100, replacement=True) means that you will sample 100 from the distrubtion independently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23845568",
      "metadata": {
        "id": "23845568"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "'''\n",
        "Example of sampling a result from a given probability distribution\n",
        "'''\n",
        "\n",
        "dummy_prob_distribution = torch.Tensor([0.1, 0.5, 0.2, 0.05, 0.15])\n",
        "sampled_out = dummy_prob_distribution.multinomial(num_samples=10000, replacement=True)\n",
        "print(sampled_out[:20])\n",
        "Counter(sampled_out.tolist()) # Number of each category sampled is almost same as num_samples * probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15fef28a",
      "metadata": {
        "scrolled": true,
        "id": "15fef28a"
      },
      "outputs": [],
      "source": [
        "def predict_single_step(model, cur_input, prev_hidden):\n",
        "  '''\n",
        "  This function runs MelodyLangaugeModel just for one step, for the given current input and previous hidden state.\n",
        "\n",
        "  Arguments:\n",
        "    model (MelodyLanguageModel)\n",
        "    cur_input (torch.LongTensor): Input for the current time step. Has a shape of (batch_size=1, 1 (timestep), 2)\n",
        "    prev_hidden (torch.Tensor): Hidden state of RNN after previous timestep\n",
        "\n",
        "  Returns:\n",
        "    cur_output (torch.LongTensor): Sampled note [pitch_category_idx, duration_category_idx] from the predicted probability distribution, with shape of [1,1,2]\n",
        "    last_hidden (torch.Tensor): Hidden state of RNN\n",
        "  Think about running the model.forward() step-by-step.\n",
        "\n",
        "  input_seq → self.get_concat_embedding → self.rnn → self.final_layer → torch.softmax for [pitch, duration] → sampled [pitch, duration]\n",
        "\n",
        "  TODO: Complete this function\n",
        "  Caution: You have to use torch.multinomial(replacement=False) to sample a note from the probability distribution.\n",
        "    You can also use replacement=True, but for the automatic evaulation, please use replacement=False.\n",
        "  '''\n",
        "  return\n",
        "\n",
        "input_vec, initial_hidden = get_initial_input_and_hidden_state(model, batch_size=1)\n",
        "out_note, last_hidden = predict_single_step(model, input_vec, initial_hidden)\n",
        "print(f'out_note: \\n{out_note} \\n last_hidden: \\n {last_hidden}')\n",
        "\n",
        "assert out_note.ndim == 3\n",
        "assert last_hidden.ndim == 3\n",
        "assert out_note.shape == (1,1,2)\n",
        "\n",
        "assert len(set([predict_single_step(model, input_vec, initial_hidden)[0] for i in range(5)]))==5, 'Generated output has to be different based on random sampling'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d87107f",
      "metadata": {
        "id": "5d87107f"
      },
      "outputs": [],
      "source": [
        "def is_end_token(model, cur_output):\n",
        "  '''\n",
        "  During the generation, there is a possibility that the generated note predicted 'end' token for either pitch or duration.\n",
        "  (In fact, model can even estimate 'start' token during the generation even though it has very low probability)\n",
        "\n",
        "  Using information among (model.pitch2idx, model.dur2idx, model.idx2pitch, model.idx2dur) to check whether the given cur_output has 'end' token or not.\n",
        "\n",
        "  Arguments:\n",
        "    model (MelodyLanguageModel)\n",
        "    cur_output (torch.LongTensor): Assume it has shape of [1,1,2 (pitch_idx, duration_idx)]\n",
        "\n",
        "  Return:\n",
        "    is_end_token (bool): True if cur_output include category index such as 'start' or 'end',\n",
        "                          else False.\n",
        "\n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "print(is_end_token(model, out_note))\n",
        "\n",
        "assert not is_end_token(model, torch.LongTensor([[[10, 7]]])), 'This is not end token'\n",
        "assert is_end_token(model, torch.LongTensor([[[2, 40]]])), 'This is end token'\n",
        "assert is_end_token(model, torch.LongTensor([[[25, 2]]])),  'This is end token'\n",
        "assert is_end_token(model, torch.LongTensor([[[2, 2]]])),  'This is end token'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c079ad9f",
      "metadata": {
        "scrolled": true,
        "id": "c079ad9f"
      },
      "outputs": [],
      "source": [
        "model.cpu()\n",
        "\n",
        "def generate(model, random_seed=0):\n",
        "  '''\n",
        "  This function generates a new melody sequence with a given model and random_seed.\n",
        "\n",
        "  Arguments:\n",
        "    model (MelodyLanguageModel)\n",
        "    random_seed (int): Language model's inference will always generate different result, because it uses random sampling for the prediction.\n",
        "                       Therefore, if you want to reproduce the same generation result, you have to fix random_seed.\n",
        "\n",
        "  Returns:\n",
        "    generated_note_sequence (torch.LongTensor): Has a shape of [num_generated_notes, 2]\n",
        "\n",
        "  TODO: Complete this function using get_initial_input_and_hidden_state(), predict_single_step(), is_end_token()\n",
        "\n",
        "  Hint: You can use while loop\n",
        "        You have to track the generated single note in a list or somewhere.\n",
        "  '''\n",
        "\n",
        "  torch.manual_seed(random_seed) # To reproduce the result, we have to control random sequence\n",
        "\n",
        "  '''\n",
        "  Write your code from here\n",
        "  '''\n",
        "\n",
        "  return\n",
        "gen_out = generate(model)\n",
        "print(f\"gen_out: \\n {gen_out}\")\n",
        "\n",
        "assert isinstance(gen_out, torch.LongTensor), f\"output of generate() has to be torch.LongTensor, not {type(gen_out)}\"\n",
        "assert gen_out.ndim == 2, f\"output of generate() has to be 2D tensor, not {gen_out.ndim}D tensor\"\n",
        "assert gen_out.shape[1] == 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be787bda",
      "metadata": {
        "id": "be787bda"
      },
      "source": [
        "### Problem 7-2. Convert neural network's prediction to music score (10 pts)\n",
        "- Even though neural network has succeeded in generating a new sequence, it is just a sequence of index that neural network uses\n",
        "    - For example, generated note event [17, 10] means that this note has pitch value of 17th pitch category and duration value of 10th duration category\n",
        "- We have to convert categorical index to original value\n",
        "    - We saved this information as `idx2pitch`, `idx2dur` while we declared the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f6f18b2",
      "metadata": {
        "id": "3f6f18b2"
      },
      "outputs": [],
      "source": [
        "def convert_idx_pred_to_origin(pred:torch.Tensor, idx2pitch:list, idx2dur:list):\n",
        "  '''\n",
        "  This function convert neural net's output index to original pitch value (MIDI Pitch) and duration value\n",
        "\n",
        "  Argument:\n",
        "    pred: generated output of the model. Has a shape of [num_notes, 2].\n",
        "          0th dimension of each note represents pitch category index\n",
        "          and 1st dimension of each note represents duration category index\n",
        "\n",
        "  Return:\n",
        "    converted_out (torch.Tensor): Has a same shape with 'pred'.\n",
        "\n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "\n",
        "  return\n",
        "\n",
        "converted_out = convert_idx_pred_to_origin(gen_out, model.idx2pitch, model.idx2dur)\n",
        "assert converted_out.shape == gen_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be29831",
      "metadata": {
        "id": "2be29831"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "To solve the next problem, you have to know how note_representation looks like in muspy.\n",
        "\n",
        "In note representation, each note is represented as [start_timestep, pitch, duration, velocity]\n",
        "\n",
        "'''\n",
        "\n",
        "note_repr_example = train_set.dataset[0]\n",
        "note_repr_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e067efdf",
      "metadata": {
        "scrolled": true,
        "id": "e067efdf"
      },
      "outputs": [],
      "source": [
        "def convert_pitch_dur_to_note_representation(pitch_dur:torch.LongTensor):\n",
        "  '''\n",
        "  This function takes pitch_dur (shape of [num_notes, 2]) and returns the corresponding note representation (shape of [num_notes, 4])\n",
        "  In note representation, each note is represented as [start_timestep, pitch, duration, velocity]\n",
        "\n",
        "  Since our generation is monophonic, you can regard start_timestep starts from 0 and accumulate the duration of note.\n",
        "  You can fix velocity to 64.\n",
        "\n",
        "\n",
        "  Arguments:\n",
        "    pitch_dur: LongTensor of note where each note represented as pitch and duration value\n",
        "\n",
        "  return:\n",
        "    note_repr: numpy.Array with shape of [num_notes, 4]\n",
        "               each note has value of [start_timestep, pitch, duration, velocity]\n",
        "\n",
        "  TODO: Complete this function\n",
        "  Hint: You can use torch.cumsum() to accumulate the duration.\n",
        "  To convert torch tensor to numpy, you can use atensor.numpy()\n",
        "\n",
        "  '''\n",
        "\n",
        "  return\n",
        "\n",
        "note_repr = convert_pitch_dur_to_note_representation(converted_out)\n",
        "note_repr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "869368b0",
      "metadata": {
        "id": "869368b0"
      },
      "source": [
        "# Submission\n",
        "- You have to copy and paste your code to ``MIR_Assignment_3.py`` file\n",
        "- Check that your code runs without error by running ``MIR_Assignment_3.py``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0b73bf",
      "metadata": {
        "id": "4e0b73bf"
      },
      "outputs": [],
      "source": [
        "!python3 MIR_Assignment_3.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b66d9303",
      "metadata": {
        "id": "b66d9303"
      },
      "source": [
        "## Generation: Visualize and synthesize the generated result (10 pts)\n",
        "- Try to generate different melody using different `random_seed`\n",
        "- In your submission, include **Three** examples of your favorite among the generated results in wav\n",
        "    - You have to install soundfont and music font using\n",
        "        - `muspy.download_bravura_font()`\n",
        "        - `muspy.download_musescore_soundfont()`\n",
        "    - You may need fluidsynth to synthesize the sound.\n",
        "        - In colab, `!sudo apt-get install fluidsynth` will work\n",
        "        - In other Ubuntu os, `sudo apt-get update` and then `sudo apt-get install fluidsynth` will work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d71a3d7",
      "metadata": {
        "scrolled": true,
        "id": "0d71a3d7"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "gen_music = muspy.from_note_representation(note_repr)\n",
        "gen_music.show_score()\n",
        "\n",
        "gen_audio = gen_music.synthesize().T\n",
        "ipd.Audio(gen_audio/2**15, rate=44100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44f9a3cb",
      "metadata": {
        "id": "44f9a3cb"
      },
      "source": [
        "- Try with different random seed and generate interesting melodies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f300bd4b",
      "metadata": {
        "id": "f300bd4b"
      },
      "outputs": [],
      "source": [
        "def generate_muspy_music(model, random_seed=0):\n",
        "  '''\n",
        "  This function combines 'generate', 'convert_idx_pred_to_origin', 'convert_pitch_dur_to_note_representation', muspy.from_note_representation\n",
        "  '''\n",
        "  gen_out = generate(model, random_seed)\n",
        "  converted_out = convert_idx_pred_to_origin(gen_out, model.idx2pitch, model.idx2dur)\n",
        "  note_repr = convert_pitch_dur_to_note_representation(converted_out)\n",
        "  gen_music = muspy.from_note_representation(note_repr)\n",
        "  return gen_music\n",
        "\n",
        "gen_music = generate_muspy_music(model, random_seed=2)\n",
        "gen_music.show_score()\n",
        "gen_audio = gen_music.synthesize().T\n",
        "ipd.Audio(gen_audio/2**15, rate=44100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ddb9b6",
      "metadata": {
        "id": "36ddb9b6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You can save audio as wave file with muspy.write_audio\n",
        "'''\n",
        "gen_music.write_audio('result_0.wav')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}